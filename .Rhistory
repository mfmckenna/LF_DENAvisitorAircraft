#outputs
# a per site .csv file: E:\RESEARCH\SanctSound\data2\combineFiles1_SPLShips
# summary table for average of all data per site (copied here):
# https://docs.google.com/spreadsheets/d/1dw9Vy0dXKW3Q6Ter3t19cmLf_5EJWbmL9nqJsCrNXQE/edit#gid=888829761
#-----------------------------------------------------------------------------------------
# TO ADD (future versions)
#-----------------------------------------------------------------------------------------
# Done! output table: add range and save out
# Done! duration of quiet- hour/daily events and average duration s, using vessel detection data
# check for vessel detection data-- if not available need to reflect this in data or if no detections
rm(list=ls())
#-----------------------------------------------------------------------------------------
# LOAD Libraries
#-----------------------------------------------------------------------------------------
library(lubridate)
library(dplyr)
library(ggplot2)
library(data.table)
library(gridExtra)
library(scales)
library(BBmisc)
library(zoo)
library(plotly)  #https://www.r-graph-gallery.com/interactive-charts.html
#-----------------------------------------------------------------------------------------
# SETUP parameters
#-----------------------------------------------------------------------------------------
ra = 7 #days for running average
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
stdize = function(x, ...) {(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
#-----------------------------------------------------------------------------------------
# OUTPUT details
#-----------------------------------------------------------------------------------------
tDir = "E:\\RESEARCH\\SanctSound\\"
outDir =  paste0(tDir,"data2\\combineFiles1_SPLShips\\")
DC = Sys.Date()
#range of dates for output graphics
eDatePlot = '2020-12-31'
sDatePlot = '2018-11-01'
#some analysis and output flags
flagCSV  = FALSE  #true if you want to output hourly and daily csv files per site
flagPLT  = FALSE  #true if you want to output summary plots of data
flagYear = TRUE   #true writes out a csv of yearly data to be copied into google docs
#-----------------------------------------------------------------------------------------
# READ IN-- vessel detection data
# note: some deployments do not have vessel detection and therefor no files
#-----------------------------------------------------------------------------------------
dirVD     = (paste0(tDir,"data2\\SanctSound_VesselDetection_DataProducts") )
nFilesVD  = length( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD =     ( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD = basename(inFilesVD)
x = strsplit(inFilesVD,"_")
sitesVD = unique(sapply( x, "[", 2 ))
cat(nFilesVD, " files: ", length(sitesVD), " number of sites with vessel detection data", "\n")
rm(x)
#-----------------------------------------------------------------------------------------
# READS IN-- AIS data
#-----------------------------------------------------------------------------------------
dirAIS      = paste0(tDir,"data")
Fpattern = "_2018_10_to_2021_04.csv"
nFilesAIS   = length( list.files(path=dirAIS,pattern = Fpattern, full.names=TRUE, recursive = TRUE))
inFilesAIS  =       ( list.files(path=dirAIS,pattern = Fpattern, full.names=TRUE, recursive = TRUE))
x = strsplit(inFilesAIS,"_")
sanctsAIS = unique(sapply( x, "[", 2 ))
#combine AIS files
multmerge = function(path){
filenames = list.files(path=path, pattern = "_2018_10_to_2020_11.csv", full.names=TRUE, recursive = TRUE)
rbindlist(lapply(filenames, fread))
}
AIS <- multmerge(dirAIS)
sitesAIS = unique(AIS$LOC_ID)
cat(length(sitesAIS), " number of sites with AIS data", "\n")
rm(x)
#-----------------------------------------------------------------------------------------
# PROCESS and COMBINE FILES by site, reads in SPL date when processing each site
#-----------------------------------------------------------------------------------------
output  = NULL
output2 = NULL #truncate to a give time period
output3 = NULL #summary of data in a specific time period, entire year 2019
ss = 24
cat("Processing site...", sitesVD[ss], ":",ss, "of", length(sitesVD),"\n")
sFiles = list.files(path=dirVD, pattern = paste0(sitesVD[ss],".*hips.csv"), full.names=TRUE, recursive = TRUE)
aData  = AIS[ AIS$LOC_ID == sitesVD[ss], ]
aData$Day = as.Date(aData$DATE,format = "%m/%d/%Y") #AIS is daily resolution
deply = sitesVD[ss]
sanct = substr(sitesVD[ss],1,2)
#-----------------------------------------------------------------------------------------
#GET SPL files-- this give us accurate date range for detections
#---------------------------------------------------------------------------------------
dirSPL  = paste0(tDir,"data\\",sanct,"\\", deply)
nFiles  = length( list.files(path=dirSPL, pattern = "_OL_1h.csv", full.names=TRUE, recursive = TRUE) )
#CHECK: are there OL SPL files??
if (nFiles == 0 ){
cat("No SPL files for ", sitesVD[ss], "\n")
#UPDATE FOR SITE WITH NO SPL DATA!!
output = rbind(output, c(sitesVD[ss], length(sFiles), nFiles,
NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA,NA,NA ) )
next
}else {
cat(nFiles, "SPL files for ",sitesVD[ss],"\n") }
#COMBINE ALL OL DATA TOGETHER, add deployment
SPL = NULL
for (ff in 1 : nFiles){
fname  = list.files(path=dirSPL, pattern = "_OL_1h.csv", full.names=FALSE, recursive = TRUE)[ff]
fnameF = list.files(path=dirSPL, pattern = "_OL_1h.csv", full.names=TRUE, recursive = TRUE)[ff]
dname  = sapply (strsplit( fname, "_" ),"[",4 )
tmp    = rbind(fread(fnameF))
tmp$deploy = dname
if (ff > 1) { names(tmp) <- NULL }
SPL = rbind(SPL,tmp)
rm(fname,fnameF,tmp)
}
#CHECK: fix format changes in SPL data headings
SPL$DateF = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPL$`yyyy-mm-ddTHH:MM:SSZ`)), tz = "GMT" )
SPL$Day   = as.Date(SPL$DateF)
if ( is.na( SPL$DateF [1]) ){#try another format!!
SPL$DateF = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPL$yyyy_mm_ddTHH_MM_SSZ)), tz = "GMT" )
SPL$Day   = as.Date(SPL$DateF)
}
#CHECK: fix some sites have different OB-- SB01 for sure!! starts at 16 Hz
#cat( sitesVD[ss], ":", colnames(SPL), "\n")
if (sitesVD[ss]== "SB01") {
cat("SPL data has extra column(s)... check!", "\n")
SPL = SPL[,c(1,3:15)]
}
if (sitesVD[ss]== "SB02") {
cat("SPL data has extra column(s)... check!", "\n")
SPL = SPL[,c(1,3:15)]
}
if (sitesVD[ss]== "SB03") {
cat("SPL data has extra column(s)... check!", "\n")
SPL = SPL[,c(1,3:15)]
}
cat( sitesVD[ss], ":", colnames(SPL), "\n")
#APPEND BB data to OL
nFiles = length( list.files(path=dirSPL,pattern = "_BB_1h.csv", full.names=TRUE, recursive = TRUE))
SPLBB = NULL
for (ff in 1 : nFiles){
fname  = list.files(path=dirSPL, pattern = "_BB_1h.csv", full.names=FALSE, recursive = TRUE)[ff]
fnameF = list.files(path=dirSPL, pattern = "_BB_1h.csv", full.names=TRUE, recursive = TRUE)[ff]
dname  = sapply (strsplit( fname, "_" ),"[",4 )
tmp    = rbind(fread(fnameF))
tmp$deploy = dname
if (ff > 1) { names(tmp) <- NULL }
SPLBB = rbind(SPLBB,tmp)
rm(fname,fnameF,tmp)
}
#GRRR DIFFERENT HEADING FORMATS!!!
colcheck = colnames(SPLBB)
if (colcheck[1] == "yyyy_mm_ddTHH_MM_SSZ" ){
#SPLBB$yyyy_mm_ddTHH_MM_SSZ
SPLBB$DateF     = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPLBB$`yyyy_mm_ddTHH_MM_SSZ`)), tz = "GMT" )
SPLBB$DateFday  = as.Date(SPL$DateF)
}else if (colcheck[1] == "yyyy-mm-ddTHH:MM:SSZ" ){
#SPLBB$`yyyy-mm-ddTHH:MM:SSZ`
SPLBB$DateF     = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPLBB$`yyyy-mm-ddTHH:MM:SSZ`)), tz = "GMT" )
SPLBB$DateFday  = as.Date(SPL$DateF)
}else { (cat ('BB heading format not found',"\n")) }
#only keep specific columns before combining
SPLBB = SPLBB[,c(2,4)]
#CHECK: duplicated rows in the SPL data!!! keep track and then remove.
duOL = nrow(SPL) - length( unique(SPL$DateF) )
duBB = nrow(SPLBB) - length( unique(SPLBB$DateF) )
#cat(sitesVD[ss], ": duplicated rows = ", duOL, "OL, ", duBB, "BB", "\n")
SPL =  SPL[!duplicated(SPL$DateF),]
SPLBB =  SPLBB[!duplicated(SPLBB$DateF),]
SPLa = merge(SPL, SPLBB, by = "DateF" )
SPL = SPLa
#change some of the colnames... because different! #as.data.frame( colnames(output))
#as.data.frame( colnames(SPL))
colnames(SPL)[3]  =  "OL_31.5"
colnames(SPL)[15] = "BB_20-24000"
#CREATE accurate time ranges for data-- hourly
#---------------------------------------------------------------------------------------
beginTime =   as.POSIXct( min(SPL$DateF) )
endTime   =   as.POSIXct( max(SPL$DateF) )
beginTime
endTime
#CREATE accurate time ranges for data-- hourly
#---------------------------------------------------------------------------------------
beginTime =   as.POSIXct( min(SPL$DateF) )
endTime   =   as.POSIXct( max(SPL$DateF) )
VESSfor   =   as.data.frame( seq(from=beginTime, to=endTime, by="hour") )
colnames(VESSfor) = "DateF"
VESSfor
SPL
# process sanctuary sound vessel detection data and AIS results
#developed for vessel metric analysis and story map
#some results copied here: https://docs.google.com/spreadsheets/d/1dw9Vy0dXKW3Q6Ter3t19cmLf_5EJWbmL9nqJsCrNXQE/edit#gid=1545323104
#-----------------------------------------------------------------------------------------
# MAIN functions-- by site processing
#-----------------------------------------------------------------------------------------
# Reads in both vessel detection and AIS data products
# Calculates daily vessel metrics per day:
# Converts vessel detection files: for each each hour, how many unique vessels detected, and percent of hour dominated by vessel noise
# some of the time will spill into the next hour, need to keep track of this
# reformat so for each hour there is a total vessel time value and total vessel count
#outputs
# a per site .csv file: E:\RESEARCH\SanctSound\data2\combineFiles1_SPLShips
# summary table for average of all data per site (copied here):
# https://docs.google.com/spreadsheets/d/1dw9Vy0dXKW3Q6Ter3t19cmLf_5EJWbmL9nqJsCrNXQE/edit#gid=888829761
#-----------------------------------------------------------------------------------------
# TO ADD (future versions)
#-----------------------------------------------------------------------------------------
# Done! output table: add range and save out
# Done! duration of quiet- hour/daily events and average duration s, using vessel detection data
# check for vessel detection data-- if not available need to reflect this in data or if no detections
rm(list=ls())
#-----------------------------------------------------------------------------------------
# LOAD Libraries
#-----------------------------------------------------------------------------------------
library(lubridate)
library(dplyr)
library(ggplot2)
library(data.table)
library(gridExtra)
library(scales)
library(BBmisc)
library(zoo)
library(plotly)  #https://www.r-graph-gallery.com/interactive-charts.html
#-----------------------------------------------------------------------------------------
# SETUP parameters
#-----------------------------------------------------------------------------------------
ra = 7 #days for running average
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
stdize = function(x, ...) {(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
#-----------------------------------------------------------------------------------------
# OUTPUT details
#-----------------------------------------------------------------------------------------
tDir = "E:\\RESEARCH\\SanctSound\\"
outDir =  paste0(tDir,"data2\\combineFiles1_SPLShips\\")
DC = Sys.Date()
#range of dates for output graphics
eDatePlot = '2020-12-31'
sDatePlot = '2018-11-01'
#some analysis and output flags
flagCSV  = FALSE  #true if you want to output hourly and daily csv files per site
flagPLT  = FALSE  #true if you want to output summary plots of data
flagYear = TRUE   #true writes out a csv of yearly data to be copied into google docs
#-----------------------------------------------------------------------------------------
# READ IN-- vessel detection data
# note: some deployments do not have vessel detection and therefor no files
#-----------------------------------------------------------------------------------------
dirVD     = (paste0(tDir,"data2\\SanctSound_VesselDetection_DataProducts") )
nFilesVD  = length( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD =     ( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD = basename(inFilesVD)
x = strsplit(inFilesVD,"_")
sitesVD = unique(sapply( x, "[", 2 ))
cat(nFilesVD, " files: ", length(sitesVD), " number of sites with vessel detection data", "\n")
rm(x)
#-----------------------------------------------------------------------------------------
# READS IN-- AIS data
#-----------------------------------------------------------------------------------------
dirAIS      = paste0(tDir,"data")
Fpattern = "_2018_10_to_2021_04.csv"
nFilesAIS   = length( list.files(path=dirAIS,pattern = Fpattern, full.names=TRUE, recursive = TRUE))
inFilesAIS  =       ( list.files(path=dirAIS,pattern = Fpattern, full.names=TRUE, recursive = TRUE))
x = strsplit(inFilesAIS,"_")
sanctsAIS = unique(sapply( x, "[", 2 ))
#combine AIS files
multmerge = function(path){
filenames = list.files(path=path, pattern = "_2018_10_to_2020_11.csv", full.names=TRUE, recursive = TRUE)
rbindlist(lapply(filenames, fread))
}
AIS <- multmerge(dirAIS)
sitesAIS = unique(AIS$LOC_ID)
cat(length(sitesAIS), " number of sites with AIS data", "\n")
rm(x)
#-----------------------------------------------------------------------------------------
# PROCESS and COMBINE FILES by site, reads in SPL date when processing each site
#-----------------------------------------------------------------------------------------
output  = NULL
output2 = NULL #truncate to a give time period
output3 = NULL #summary of data in a specific time period, entire year 2019
ss = 24
cat("Processing site...", sitesVD[ss], ":",ss, "of", length(sitesVD),"\n")
sFiles = list.files(path=dirVD, pattern = paste0(sitesVD[ss],".*hips.csv"), full.names=TRUE, recursive = TRUE)
aData  = AIS[ AIS$LOC_ID == sitesVD[ss], ]
aData$Day = as.Date(aData$DATE,format = "%m/%d/%Y") #AIS is daily resolution
deply = sitesVD[ss]
sanct = substr(sitesVD[ss],1,2)
#-----------------------------------------------------------------------------------------
#GET SPL files-- this give us accurate date range for detections
#---------------------------------------------------------------------------------------
dirSPL  = paste0(tDir,"data\\",sanct,"\\", deply)
nFiles  = length( list.files(path=dirSPL, pattern = "_OL_1h.csv", full.names=TRUE, recursive = TRUE) )
#CHECK: are there OL SPL files??
if (nFiles == 0 ){
cat("No SPL files for ", sitesVD[ss], "\n")
#UPDATE FOR SITE WITH NO SPL DATA!!
output = rbind(output, c(sitesVD[ss], length(sFiles), nFiles,
NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA, NA,NA,NA,NA,NA,NA ) )
next
}else {
cat(nFiles, "SPL files for ",sitesVD[ss],"\n") }
#COMBINE ALL OL DATA TOGETHER, add deployment
SPL = NULL
for (ff in 1 : nFiles){
fname  = list.files(path=dirSPL, pattern = "_OL_1h.csv", full.names=FALSE, recursive = TRUE)[ff]
fnameF = list.files(path=dirSPL, pattern = "_OL_1h.csv", full.names=TRUE, recursive = TRUE)[ff]
dname  = sapply (strsplit( fname, "_" ),"[",4 )
tmp    = rbind(fread(fnameF))
tmp$deploy = dname
if (ff > 1) { names(tmp) <- NULL }
SPL = rbind(SPL,tmp)
rm(fname,fnameF,tmp)
}
#CHECK: fix format changes in SPL data headings
SPL$DateF = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPL$`yyyy-mm-ddTHH:MM:SSZ`)), tz = "GMT" )
SPL$Day   = as.Date(SPL$DateF)
if ( is.na( SPL$DateF [1]) ){#try another format!!
SPL$DateF = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPL$yyyy_mm_ddTHH_MM_SSZ)), tz = "GMT" )
SPL$Day   = as.Date(SPL$DateF)
}
#CHECK: fix some sites have different OB-- SB01 for sure!! starts at 16 Hz
#cat( sitesVD[ss], ":", colnames(SPL), "\n")
if (sitesVD[ss]== "SB01") {
cat("SPL data has extra column(s)... check!", "\n")
SPL = SPL[,c(1,3:15)]
}
if (sitesVD[ss]== "SB02") {
cat("SPL data has extra column(s)... check!", "\n")
SPL = SPL[,c(1,3:15)]
}
if (sitesVD[ss]== "SB03") {
cat("SPL data has extra column(s)... check!", "\n")
SPL = SPL[,c(1,3:15)]
}
cat( sitesVD[ss], ":", colnames(SPL), "\n")
#APPEND BB data to OL
nFiles = length( list.files(path=dirSPL,pattern = "_BB_1h.csv", full.names=TRUE, recursive = TRUE))
SPLBB = NULL
for (ff in 1 : nFiles){
fname  = list.files(path=dirSPL, pattern = "_BB_1h.csv", full.names=FALSE, recursive = TRUE)[ff]
fnameF = list.files(path=dirSPL, pattern = "_BB_1h.csv", full.names=TRUE, recursive = TRUE)[ff]
dname  = sapply (strsplit( fname, "_" ),"[",4 )
tmp    = rbind(fread(fnameF))
tmp$deploy = dname
if (ff > 1) { names(tmp) <- NULL }
SPLBB = rbind(SPLBB,tmp)
rm(fname,fnameF,tmp)
}
#GRRR DIFFERENT HEADING FORMATS!!!
colcheck = colnames(SPLBB)
if (colcheck[1] == "yyyy_mm_ddTHH_MM_SSZ" ){
#SPLBB$yyyy_mm_ddTHH_MM_SSZ
SPLBB$DateF     = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPLBB$`yyyy_mm_ddTHH_MM_SSZ`)), tz = "GMT" )
SPLBB$DateFday  = as.Date(SPL$DateF)
}else if (colcheck[1] == "yyyy-mm-ddTHH:MM:SSZ" ){
#SPLBB$`yyyy-mm-ddTHH:MM:SSZ`
SPLBB$DateF     = as.POSIXct( gsub(".000Z", "", gsub("T", " ", SPLBB$`yyyy-mm-ddTHH:MM:SSZ`)), tz = "GMT" )
SPLBB$DateFday  = as.Date(SPL$DateF)
}else { (cat ('BB heading format not found',"\n")) }
#only keep specific columns before combining
SPLBB = SPLBB[,c(2,4)]
#CHECK: duplicated rows in the SPL data!!! keep track and then remove.
duOL = nrow(SPL) - length( unique(SPL$DateF) )
duBB = nrow(SPLBB) - length( unique(SPLBB$DateF) )
#cat(sitesVD[ss], ": duplicated rows = ", duOL, "OL, ", duBB, "BB", "\n")
SPL =  SPL[!duplicated(SPL$DateF),]
SPLBB =  SPLBB[!duplicated(SPLBB$DateF),]
SPLa = merge(SPL, SPLBB, by = "DateF" )
SPL = SPLa
#change some of the colnames... because different! #as.data.frame( colnames(output))
#as.data.frame( colnames(SPL))
colnames(SPL)[3]  =  "OL_31.5"
colnames(SPL)[15] = "BB_20-24000"
#CREATE accurate time ranges for data-- hourly
#---------------------------------------------------------------------------------------
beginTime =   as.POSIXct( min(SPL$DateF) )
endTime   =   as.POSIXct( max(SPL$DateF) )
VESSfor   =   as.data.frame( seq(from=beginTime, to=endTime, by="hour") )
colnames(VESSfor) = "DateF"
FullListTimes = merge(VESSfor, SPL, by="DateF", all = TRUE)
FullListTimes$Site = sitesVD[ss]
FullListTimes = FullListTimes[,c(16,13,14,1,3:12,15)] #reorder and truncate!
FullListTimes$Day = as.Date(FullListTimes$DateF)
FullListTimes
View(FullListTimes)
View(SPL)
# process sanctuary sound vessel detection data and AIS results
#developed for vessel metric analysis and story map
#some results copied here: https://docs.google.com/spreadsheets/d/1dw9Vy0dXKW3Q6Ter3t19cmLf_5EJWbmL9nqJsCrNXQE/edit#gid=1545323104
#-----------------------------------------------------------------------------------------
# MAIN functions-- by site processing
#-----------------------------------------------------------------------------------------
# Reads in both vessel detection and AIS data products
# Calculates daily vessel metrics per day:
# Converts vessel detection files: for each each hour, how many unique vessels detected, and percent of hour dominated by vessel noise
# some of the time will spill into the next hour, need to keep track of this
# reformat so for each hour there is a total vessel time value and total vessel count
#outputs
# a per site .csv file: E:\RESEARCH\SanctSound\data2\combineFiles1_SPLShips
# summary table for average of all data per site (copied here):
# https://docs.google.com/spreadsheets/d/1dw9Vy0dXKW3Q6Ter3t19cmLf_5EJWbmL9nqJsCrNXQE/edit#gid=888829761
#-----------------------------------------------------------------------------------------
# TO ADD (future versions)
#-----------------------------------------------------------------------------------------
# Done! output table: add range and save out
# Done! duration of quiet- hour/daily events and average duration s, using vessel detection data
# check for vessel detection data-- if not available need to reflect this in data or if no detections
rm(list=ls())
#-----------------------------------------------------------------------------------------
# LOAD Libraries
#-----------------------------------------------------------------------------------------
library(lubridate)
library(dplyr)
library(ggplot2)
library(data.table)
library(gridExtra)
library(scales)
library(BBmisc)
library(zoo)
library(plotly)  #https://www.r-graph-gallery.com/interactive-charts.html
#-----------------------------------------------------------------------------------------
# SETUP parameters
#-----------------------------------------------------------------------------------------
ra = 7 #days for running average
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
stdize = function(x, ...) {(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
#-----------------------------------------------------------------------------------------
# OUTPUT details
#-----------------------------------------------------------------------------------------
tDir = "E:\\RESEARCH\\SanctSound\\"
outDir =  paste0(tDir,"data2\\combineFiles1_SPLShips\\")
DC = Sys.Date()
#range of dates for output graphics
eDatePlot = '2020-12-31'
sDatePlot = '2018-11-01'
#some analysis and output flags
flagCSV  = FALSE  #true if you want to output hourly and daily csv files per site
flagPLT  = FALSE  #true if you want to output summary plots of data
flagYear = TRUE   #true writes out a csv of yearly data to be copied into google docs
#-----------------------------------------------------------------------------------------
# READ IN-- vessel detection data
# note: some deployments do not have vessel detection and therefor no files
#-----------------------------------------------------------------------------------------
dirVD     = (paste0(tDir,"data2\\SanctSound_VesselDetection_DataProducts") )
nFilesVD  = length( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD =     ( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD = basename(inFilesVD)
x = strsplit(inFilesVD,"_")
sitesVD = unique(sapply( x, "[", 2 ))
cat(nFilesVD, " files: ", length(sitesVD), " number of sites with vessel detection data", "\n")
rm(x)
#-----------------------------------------------------------------------------------------
# READS IN-- AIS data
#-----------------------------------------------------------------------------------------
dirAIS      = paste0(tDir,"data")
Fpattern = "_2018_10_to_2021_04.csv"
nFilesAIS   = length( list.files(path=dirAIS,pattern = Fpattern, full.names=TRUE, recursive = TRUE))
inFilesAIS  =       ( list.files(path=dirAIS,pattern = Fpattern, full.names=TRUE, recursive = TRUE))
x = strsplit(inFilesAIS,"_")
sanctsAIS = unique(sapply( x, "[", 2 ))
#combine AIS files
multmerge = function(path){
filenames = list.files(path=path, pattern = "_2018_10_to_2020_11.csv", full.names=TRUE, recursive = TRUE)
rbindlist(lapply(filenames, fread))
}
AIS <- multmerge(dirAIS)
sitesAIS = unique(AIS$LOC_ID)
cat(length(sitesAIS), " number of sites with AIS data", "\n")
rm(x)
#-----------------------------------------------------------------------------------------
# PROCESS and COMBINE FILES by site, reads in SPL date when processing each site
#-----------------------------------------------------------------------------------------
output  = NULL
output2 = NULL #truncate to a give time period
output3 = NULL #summary of data in a specific time period, entire year 2019
rm(list=ls())
#-----------------------------------------------------------------------------------------
# LOAD Libraries
#-----------------------------------------------------------------------------------------
library(lubridate)
library(dplyr)
library(ggplot2)
library(data.table)
library(gridExtra)
library(scales)
library(BBmisc)
library(zoo)
library(plotly)  #https://www.r-graph-gallery.com/interactive-charts.html
#-----------------------------------------------------------------------------------------
# SETUP parameters
#-----------------------------------------------------------------------------------------
ra = 7 #days for running average
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
stdize = function(x, ...) {(x - min(x, ...)) / (max(x, ...) - min(x, ...))}
#-----------------------------------------------------------------------------------------
# OUTPUT details
#-----------------------------------------------------------------------------------------
tDir = "E:\\RESEARCH\\SanctSound\\"
outDir =  paste0(tDir,"data2\\combineFiles1_SPLShips\\")
DC = Sys.Date()
#range of dates for output graphics
eDatePlot = '2020-12-31'
sDatePlot = '2018-11-01'
#some analysis and output flags
flagCSV  = FALSE  #true if you want to output hourly and daily csv files per site
flagPLT  = FALSE  #true if you want to output summary plots of data
flagYear = TRUE   #true writes out a csv of yearly data to be copied into google docs
#-----------------------------------------------------------------------------------------
# READ IN-- vessel detection data
# note: some deployments do not have vessel detection and therefor no files
#-----------------------------------------------------------------------------------------
dirVD     = (paste0(tDir,"data2\\SanctSound_VesselDetection_DataProducts") )
nFilesVD  = length( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD =     ( list.files(path=dirVD, pattern = "*hips.csv", full.names=TRUE, recursive = TRUE))
inFilesVD = basename(inFilesVD)
x = strsplit(inFilesVD,"_")
sitesVD = unique(sapply( x, "[", 2 ))
dirVD
